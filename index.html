<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Arundev Vamadevan">

<title>Generative AI - Encoder-Decoder, Attention, and Transformers</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="index_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="index_files/libs/quarto-html/anchor.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="index_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="index_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="index_files/libs/bootstrap/bootstrap-bb462d781dde1847d9e3ccf7736099dd.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">


<link rel="stylesheet" href="footer.css">
<link rel="stylesheet" href="watermark.css">
</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Generative AI - Encoder-Decoder, Attention, and Transformers</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p><a href="https://www.linkedin.com/in/arundev-v">Arundev Vamadevan</a> </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="Images/frame.png" class="img-fluid figure-img" width="200"></p>
<figcaption>Scan to visit the Github Page</figcaption>
</figure>
</div>
<section id="generative-ai" class="level1">
<h1>Generative AI</h1>
<section id="encoder-decoder" class="level2">
<h2 class="anchored" data-anchor-id="encoder-decoder">Encoder &amp; Decoder</h2>
<p>So far, we have learned different RNN models, but they have different problems:</p>
<ol type="1">
<li><strong>Simple RNN</strong> → <em>Vanishing gradient problem</em></li>
<li><strong>LSTM-RNN</strong></li>
<li><strong>GRU-RNN</strong></li>
</ol>
<blockquote class="blockquote">
<p>LSTM (Long Short Term Memory) and GRU (Gated Reccurent Unit) have long short-term memory and are very efficient in solving problems like many-to-one RNN tasks (e.g., sentiment analysis, predicting the next word).</p>
</blockquote>
<hr>
</section>
<section id="lstm-vs-gru-comparison" class="level2">
<h2 class="anchored" data-anchor-id="lstm-vs-gru-comparison">LSTM vs GRU Comparison</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 82%">
</colgroup>
<thead>
<tr class="header">
<th>LSTM</th>
<th>GRU (Gated Recurrent Unit)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Uses a gating mechanism with 3 gates: input, forget, output</td>
<td>Has 2 gates: update and reset</td>
</tr>
<tr class="even">
<td>Excellent for long-term dependency</td>
<td>Performs similarly with fewer parameters</td>
</tr>
<tr class="odd">
<td>Maintains context over longer periods</td>
<td>More efficient and simpler</td>
</tr>
<tr class="even">
<td>Good for vanishing gradient issues</td>
<td>Good performance and efficiency</td>
</tr>
</tbody>
</table>
<hr>
</section>
<section id="limitation-of-lstm-and-gru---they-lacks-future-context" class="level2">
<h2 class="anchored" data-anchor-id="limitation-of-lstm-and-gru---they-lacks-future-context">Limitation of LSTM and GRU - They lacks future context</h2>
<section id="reason-unidirectional-processing" class="level4">
<h4 class="anchored" data-anchor-id="reason-unidirectional-processing">Reason : Unidirectional Processing</h4>
<p>LSTM &amp; GRU lack future context due to one-way processing. Both process sequence in only one direction - from left to right (or from past to future). ie when making a prediction at position t, they only have access to information from 0 to t, not from t+1 onwards.</p>
<p><strong>Example:</strong></p>
<blockquote class="blockquote">
<p>“The person who treats patients in the _____ is a doctor.”</p>
</blockquote>
<p>A standard LSTM / GRU processing the blank would not yet know that “doctor” is coming, which would be a strong clue that “hospital” should fill the blank.</p>
</section>
<section id="solution-bidirectional-rnns" class="level3">
<h3 class="anchored" data-anchor-id="solution-bidirectional-rnns">Solution: Bidirectional RNNs</h3>
<p>Works by running two separate RNNs (can be LSTM or GRU cells) simultaneously in opposite directions and their outputs are combined.</p>
</section>
</section>
<section id="bidirectional-rnn" class="level2">
<h2 class="anchored" data-anchor-id="bidirectional-rnn">Bidirectional RNN</h2>
<ul>
<li>B-RNN captures both past (forward RNN) and future (backward RNN) context.</li>
<li>Effective in translation, speech recognition, and language understanding tasks.</li>
<li>Downside: Double computational cost.</li>
<li>Not suited for real-time streaming applications.</li>
</ul>
<hr>
</section>
<section id="sequence-to-sequence-processing" class="level2">
<h2 class="anchored" data-anchor-id="sequence-to-sequence-processing">Sequence-to-Sequence Processing</h2>
<p>Example: <strong>Machine Translation (English to Hindi)</strong></p>
<p>Giving a sequence of inputs and producing a sequence of outputs. Type of RNN needed : Many to Many</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/1_Seq-To-Seq.png" class="img-fluid figure-img"></p>
<figcaption>Image - Seq2Seq Architecture</figcaption>
</figure>
</div>
<p>These kind of scenarios where sequence of inputs accepted and sequence of output should generated, Basic RNNs will struggle and will not give good results. That is why We need <strong>Encoder–Decoder Architecture</strong>.</p>
<p>Another example: chatbot<br>
&gt; Input: “Hi, how are you?” → Output: response is also a sequence.</p>
</section>
<section id="encoder-decoder-architecture" class="level2">
<h2 class="anchored" data-anchor-id="encoder-decoder-architecture">Encoder-Decoder Architecture</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/2_Enc-Dec-Arch.png" class="img-fluid figure-img"></p>
<figcaption>Image - Encoder - Decoder Architecture</figcaption>
</figure>
</div>
<section id="encoder-process" class="level3">
<h3 class="anchored" data-anchor-id="encoder-process">1. Encoder Process</h3>
<ol type="1">
<li>Processes input sequence one token at a time.</li>
<li>As it process each token, the hidden state is also updated to capture the information about the sequence.</li>
<li>After processing the entire sequences, Final hidden state = context vector.</li>
<li>This context vector represents the summarized representation of entire input sequence.</li>
</ol>
<hr>
</section>
<section id="context-vector-transfer" class="level3">
<h3 class="anchored" data-anchor-id="context-vector-transfer">2. Context Vector Transfer</h3>
<ul>
<li>Contains compressed representation of the input.</li>
<li>Bottleneck: condenses all info into one fixed-length vector.</li>
<li>Sent to decoder as initial hidden state to start the decoding process.</li>
</ul>
<hr>
</section>
<section id="decoder-process" class="level3">
<h3 class="anchored" data-anchor-id="decoder-process">3. Decoder Process</h3>
<ol type="1">
<li>Receives context vector as initial hidden state.</li>
<li>Outputs sequence (eg:- Translation) one token at a time.</li>
<li>At each step decoder uses current hidden state and previously generated token.</li>
</ol>
<p>Example : “Good Morning” → Translate to German Language</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/3_Enc-Dec-Example_1.png" class="img-fluid figure-img"></p>
<figcaption>Encoder - Decoder Example</figcaption>
</figure>
</div>
</section>
<section id="embedded-layer" class="level3">
<h3 class="anchored" data-anchor-id="embedded-layer">Embedded Layer</h3>
<ol type="1">
<li>Convert input sentence into tokens (via embedding layer or word2vec).</li>
<li>Transforms each token into a fixed-size dense vector.</li>
<li>These vectors capture word semantics.</li>
</ol>
<p>Example: Suppose we have an english vocabulary of 1000 words and each word is represented as a 300-dimensional vector. The words “Good” and “Morning” are each converted into 300 dimensional vector using embedding layer.These vectors capture meaning and relationships between words (semantics)</p>
<p>“Good” → [0.13, -0.72, 0.05, …, 0.91] (300 numbers)</p>
<p>“Morning” → [0.44, -0.11, 0.63, …, -0.21] (300 numbers)</p>
<p>300 dimension Used in Word2Vec (Google) and GloVe (Stanford) pretrained embeddings to balance performance vs computational cos, butit could be</p>
<ol type="1">
<li><p>50, 100, 200 (faster, lighter)</p></li>
<li><p>300 (balanced)</p></li>
<li><p>512 or 768 (used in BERT, GPT models)</p></li>
</ol>
<hr>
</section>
<section id="encoder" class="level3">
<h3 class="anchored" data-anchor-id="encoder">Encoder</h3>
<ul>
<li>LSTM processes input sequence.</li>
<li>Final state = context vector</li>
</ul>
</section>
<section id="decoder" class="level3">
<h3 class="anchored" data-anchor-id="decoder">Decoder</h3>
<ul>
<li>Another LSTM to generate translated output.</li>
<li>Uses context vector, previous token, and its own hidden state.</li>
</ul>
<hr>
</section>
<section id="output-layer-fully-connected-softmax" class="level3">
<h3 class="anchored" data-anchor-id="output-layer-fully-connected-softmax">Output Layer (Fully Connected Softmax)</h3>
<ul>
<li>Decoder output → Fully connected layer → Softmax : The output of LSTM at each step is passed though a fully connected dense layer to transform it into a vector of the same size as the German vocabulary.</li>
<li>Converts to probability distribution : The vector is passed through softmax function -&gt; turns into probability distribution between 0 and 1.</li>
<li>Highest value index = predicted word</li>
</ul>
</section>
</section>
<section id="limitation-of-encoderdecoder-architecture" class="level2">
<h2 class="anchored" data-anchor-id="limitation-of-encoderdecoder-architecture">Limitation of Encoder–Decoder Architecture</h2>
<ul>
<li>One fixed context vector (encoders final state) used for all outputs.</li>
<li>If sentence is long, compression leads to <strong>information loss</strong>.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/4_seq2seq.png" class="img-fluid figure-img"></p>
<figcaption>Bleu Score vs Sentence Length</figcaption>
</figure>
</div>
<p>Since it only consider the context vector in final state (not from each state), the context vector tend to have more information with nearest word which has seen in nearest time stamp. This problem is called <strong>Information Bottleneck</strong></p>
<section id="solution" class="level4">
<h4 class="anchored" data-anchor-id="solution">Solution</h4>
<p>Give equal importance to all words in input sequence regardless of relevance to the current output token.</p>
<ol type="1">
<li><p><strong>Encoder-Decoder with Attention Mechanism</strong><br>
Allows the decoder to focus on different parts of the input sequence at each decoding step.</p></li>
<li><p><strong>Transformer Architecture</strong><br>
Replaces traditional RNNs in encoder-decoder with a self-attention mechanism to better capture relationships between all words in the sequence.</p></li>
</ol>
<p>Therefore, with attention mechanism, the final one-time context vector transfer is replaced with a <strong>dynamic attention process</strong>, enabling the decoder to access different parts of the encoder output at each generation step.</p>
</section>
</section>
<section id="attention-based-encoder-decoder" class="level2">
<h2 class="anchored" data-anchor-id="attention-based-encoder-decoder">Attention-Based Encoder-Decoder</h2>
<ol type="1">
<li><strong>Encoder Output</strong>
<ul>
<li>Keep all hidden states<br>
</li>
</ul></li>
<li><strong>Alignment Score</strong>
<ul>
<li>Score between current decoder state and each encoder state<br>
</li>
</ul></li>
<li><strong>Attention Weights</strong>
<ul>
<li>Use softmax to compute probabilities<br>
</li>
</ul></li>
<li><strong>Context Vector</strong>
<ul>
<li>Weighted sum of encoder states<br>
</li>
</ul></li>
<li><strong>Output Generation</strong>
<ul>
<li>Combines context vector + previous output + decoder state</li>
</ul></li>
</ol>
<hr>
</section>
<section id="limitations-of-attention" class="level2">
<h2 class="anchored" data-anchor-id="limitations-of-attention">Limitations of Attention</h2>
<ol type="1">
<li><strong>Sequential Processing</strong> – No parallelization<br>
</li>
<li><strong>Long-Range Dependency</strong> – Still struggles with very long sequences<br>
</li>
<li><strong>Vanishing/Exploding Gradients</strong><br>
</li>
<li><strong>Computational Cost</strong></li>
</ol>
<hr>
</section>
<section id="transformers" class="level2">
<h2 class="anchored" data-anchor-id="transformers">Transformers</h2>
<section id="why-transformers" class="level3">
<h3 class="anchored" data-anchor-id="why-transformers">Why Transformers?</h3>
<ol type="1">
<li><strong>Parallelization</strong>
<ul>
<li>Processes whole sequence at once<br>
</li>
<li>Faster on GPU/TPU<br>
</li>
</ul></li>
<li><strong>Direct Connection</strong>
<ul>
<li>Self-attention allows connections between all positions<br>
</li>
<li>Solves information highway problem<br>
</li>
</ul></li>
<li><strong>Multi-head Attention</strong>
<ul>
<li>Attend to different representations<br>
</li>
<li>Capture syntax, semantics, relationships<br>
</li>
<li>Enables parallel computation</li>
</ul></li>
</ol>
<pre class="text"><code>Sentence: “The athlete who won many medals stopped competing”
- Head 1: “athlete” → “stopped” (subject – verb)
- Head 2: “athlete” → “won” (relative clause)
- Head 3: “won” → “medals” (object)</code></pre>
<hr>
<ol start="4" type="1">
<li><strong>Positional Encoding</strong>
<ul>
<li>Adds unique vectors to word embeddings for word order awareness</li>
</ul></li>
</ol>
<pre class="text"><code>"Police arrested protester" ≠ "Protester arrested police"</code></pre>
<ol start="5" type="1">
<li><strong>Scalability</strong>
<ul>
<li>Efficient for large models and datasets</li>
</ul></li>
</ol>
<hr>
</section>
</section>
<section id="context-vectors-in-transformers" class="level2">
<h2 class="anchored" data-anchor-id="context-vectors-in-transformers">Context Vectors in Transformers</h2>
<p>Contextualized representations of tokens using self-attention mechanism.</p>
<p>Steps:<br>
1. Input Embedding<br>
2. Positional Encoding<br>
3. Self-Attention<br>
4. Feedforward Layers<br>
5. Contextualized Output</p>
<hr>
</section>
<section id="contextual-embeddings-vs-static-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="contextual-embeddings-vs-static-embeddings">Contextual Embeddings vs Static Embeddings</h2>
<p><strong>Word2Vec (Static):</strong><br>
&gt; Same vector for word in all contexts<br>
→ “Light the candle” vs “Sun gives light”</p>
<p><strong>Contextual Embedding:</strong><br>
- Varies depending on sentence</p>
<p><strong>Example:</strong><br>
The person who treats people is a __________?</p>
<hr>
</section>
<section id="transformer-architecture" class="level2">
<h2 class="anchored" data-anchor-id="transformer-architecture">Transformer Architecture</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/5_transformer.png" class="img-fluid figure-img"></p>
<figcaption>Tranformer Architecture</figcaption>
</figure>
</div>
<section id="encoder-1" class="level3">
<h3 class="anchored" data-anchor-id="encoder-1">Encoder:</h3>
<ol type="1">
<li>Input Embedding + Positional Encoding<br>
</li>
<li>Self-Attention<br>
</li>
<li>Feedforward Layers<br>
</li>
<li>Encoder Output</li>
</ol>
</section>
<section id="decoder-1" class="level3">
<h3 class="anchored" data-anchor-id="decoder-1">Decoder:</h3>
<ol type="1">
<li>Output Embedding + Positional Encoding<br>
</li>
<li>Masked Self-Attention<br>
</li>
<li>Cross-Attention with Encoder Output<br>
</li>
<li>Feedforward Layers<br>
</li>
<li>Output Projection (Softmax)</li>
</ol>
<hr>
</section>
</section>
<section id="bert-vs-gpt" class="level2">
<h2 class="anchored" data-anchor-id="bert-vs-gpt">BERT vs GPT</h2>
<ul>
<li><strong>BERT</strong> – Encoder only (e.g., QA, NER)<br>
</li>
<li><strong>GPT</strong> – Decoder only (text generation)</li>
</ul>
<hr>
</section>
<section id="what-is-generative-ai" class="level2">
<h2 class="anchored" data-anchor-id="what-is-generative-ai">What is Generative AI?</h2>
<ul>
<li>Creates original content: text, image, music, code, video<br>
</li>
<li>Uses learned patterns to generate new, realistic outputs</li>
</ul>
<p><strong>Example:</strong> DALL·E – Generates new image from prompt</p>
<hr>
</section>
<section id="types-of-generative-ai-models" class="level2">
<h2 class="anchored" data-anchor-id="types-of-generative-ai-models">Types of Generative AI Models</h2>
<ol type="1">
<li>Generative Adversarial Networks (GANs)<br>
</li>
<li>Variational Autoencoders (VAEs)<br>
</li>
<li>Transformer Models (e.g., GPT)</li>
</ol>
<hr>
<section id="gans" class="level3">
<h3 class="anchored" data-anchor-id="gans">GANs</h3>
<ul>
<li>Image generation<br>
</li>
<li>Generator vs Discriminator network<br>
</li>
<li>Generator improves over time</li>
</ul>
<hr>
</section>
<section id="vaes" class="level3">
<h3 class="anchored" data-anchor-id="vaes">VAEs</h3>
<ul>
<li>Encodes input to latent space<br>
</li>
<li>Reconstructs or generates new data<br>
</li>
<li>Use case: reconstructing medical images</li>
</ul>
<hr>
</section>
<section id="transformers-1" class="level3">
<h3 class="anchored" data-anchor-id="transformers-1">Transformers</h3>
<ul>
<li>Effective in text generation<br>
</li>
<li>Self-attention for context<br>
</li>
<li>Scalable to large sequences and datasets</li>
</ul>
<hr>
</section>
</section>
<section id="large-language-models-llms" class="level2">
<h2 class="anchored" data-anchor-id="large-language-models-llms">Large Language Models (LLMs)</h2>
<ul>
<li>Trained on massive datasets<br>
</li>
<li>Understand and generate human language<br>
</li>
<li>Billions of parameters (e.g., GPT-3 = 175B)</li>
</ul>
<p><strong>Use Cases:</strong><br>
Chatbots, Q&amp;A, summarization, speech recognition</p>
<hr>
</section>
<section id="how-llms-work" class="level2">
<h2 class="anchored" data-anchor-id="how-llms-work">How LLMs Work</h2>
<section id="transformer-architecture-1" class="level3">
<h3 class="anchored" data-anchor-id="transformer-architecture-1">Transformer Architecture</h3>
<ul>
<li>Uses self-attention<br>
</li>
<li>Generates token-by-token</li>
</ul>
</section>
<section id="self-supervised-learning" class="level3">
<h3 class="anchored" data-anchor-id="self-supervised-learning">Self-Supervised Learning</h3>
<ul>
<li>Predicts next word without labels</li>
</ul>
</section>
<section id="tokenization" class="level3">
<h3 class="anchored" data-anchor-id="tokenization">Tokenization</h3>
<ul>
<li>Breaks text into tokens<br>
</li>
<li>Token limit includes input + output</li>
</ul>
<hr>
</section>
</section>
<section id="llm-providers" class="level2">
<h2 class="anchored" data-anchor-id="llm-providers">LLM Providers</h2>
<ul>
<li><strong>OpenAI</strong> – GPT-3/4, DALL·E, Whisper<br>
</li>
<li><strong>Anthropic</strong> – Claude<br>
</li>
<li><strong>Meta AI</strong> – LLaMA<br>
</li>
<li><strong>Hugging Face</strong> – BERT, T5, GPT-2<br>
</li>
<li><strong>Microsoft Azure</strong>, <strong>Google Vertex</strong>, <strong>Amazon Titan</strong></li>
</ul>
<hr>
</section>
<section id="choosing-the-right-model" class="level2">
<h2 class="anchored" data-anchor-id="choosing-the-right-model">Choosing the Right Model</h2>
<ul>
<li>Quality<br>
</li>
<li>Speed<br>
</li>
<li>Price<br>
</li>
<li>Latency</li>
</ul>
<hr>
</section>
<section id="open-source-vs-proprietary" class="level2">
<h2 class="anchored" data-anchor-id="open-source-vs-proprietary">Open Source vs Proprietary</h2>
<section id="open-source" class="level3">
<h3 class="anchored" data-anchor-id="open-source">Open Source</h3>
<ul>
<li>Free and modifiable<br>
</li>
<li>Examples: LLaMA, GPT-Neo</li>
</ul>
</section>
<section id="proprietary" class="level3">
<h3 class="anchored" data-anchor-id="proprietary">Proprietary</h3>
<ul>
<li>API only<br>
</li>
<li>Examples: GPT-4, Claude</li>
</ul>
<hr>
</section>
</section>
<section id="hugging-face-and-ollama" class="level2">
<h2 class="anchored" data-anchor-id="hugging-face-and-ollama">HUGGING FACE AND Ollama</h2>
<p>Hugging Face and Ollama serve different purposes in the LLM ecosystem:</p>
<section id="hugging-face" class="level3">
<h3 class="anchored" data-anchor-id="hugging-face">Hugging Face</h3>
<ul>
<li>A comprehensive AI platform and community hub<br>
</li>
<li>Hosts thousands of open-source models across various AI domains (not just LLMs)<br>
</li>
<li>Provides tools for model training, fine-tuning, and deployment<br>
</li>
<li>Offers both free and paid services (Inference API)<br>
</li>
<li>Includes datasets, documentation, and research papers<br>
</li>
<li>A hosting platform where users can create and share machine learning web applications with minimal setup.<br>
</li>
<li>Has enterprise-focused solutions<br>
</li>
<li>Web-based interface with extensive collaboration features<br>
</li>
<li>Focuses on being a complete MLOps platform with model cards, versioning, etc.</li>
</ul>
</section>
<section id="ollama" class="level3">
<h3 class="anchored" data-anchor-id="ollama">Ollama</h3>
<ul>
<li>Focused specifically on running LLMs locally<br>
</li>
<li>Simplified command-line interface for downloading and running open-source models<br>
</li>
<li>Prioritizes ease of use for personal computing environments<br>
</li>
<li>Optimized for desktop/laptop performance with efficient resource usage<br>
</li>
<li>Provides a simple API for local application integration<br>
</li>
<li>Includes model library management functionality<br>
</li>
<li>Emphasizes privacy by keeping all processing on your machine<br>
</li>
<li>Primarily targets individual developers and hobbyists</li>
</ul>
<p><strong>The main difference is scope and focus</strong>:<br>
Hugging Face is a comprehensive AI platform with broad capabilities, while Ollama is a specialized tool designed specifically to make running LLMs locally as simple as possible.</p>
<hr>
</section>
<section id="ollama-capabilities" class="level3">
<h3 class="anchored" data-anchor-id="ollama-capabilities">Ollama Capabilities</h3>
<ol type="1">
<li>Generate text<br>
</li>
<li>Answer a question<br>
</li>
<li>Summarize text<br>
</li>
<li>Translate text</li>
</ol>
<hr>
</section>
</section>
<section id="hugging-face-1" class="level2">
<h2 class="anchored" data-anchor-id="hugging-face-1">HUGGING FACE</h2>
<p>Hugging Face is a platform in which we can do Gen AI projects, NLP projects, and Computer Vision projects.<br>
Here we are more focusing on Gen AI tasks.</p>
<p>Hugging Face has lots of Large Language Models and Datasets that can be used to train these kinds of LLMs.<br>
Spaces – provides infrastructure for training models and deploy, but it is a paid service.</p>
<p>Hugging Face provides one pipeline, so with the help of a pipeline we can easily perform any kind of task.<br>
Hugging Face also has one Python library called <strong>Transformers</strong>, which can be used for creating any kind of Gen AI project.</p>
<p>We will use a library called <code>Transformers</code>.<br>
<strong>Transformer</strong> is the architecture but <strong>Transformers</strong> is the library.<br>
It is named because it has all the models which are made based on Transformer architecture.</p>
<p>If we want to use the Hugging Face platform with the help of Python, we need the <strong>Transformers</strong> library (It has all the functionalities to use it with Python).</p>
<section id="pipeline-function-used" class="level4">
<h4 class="anchored" data-anchor-id="pipeline-function-used">Pipeline – function used</h4>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="nlp-tasks" class="level2">
<h2 class="anchored" data-anchor-id="nlp-tasks">NLP Tasks</h2>
<section id="text-classification" class="level3">
<h3 class="anchored" data-anchor-id="text-classification">1. Text Classification</h3>
<p>Assigning a category to a piece of text (e.g., Sentiment Analysis, Topic Classification, Spam Detection):</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>classifier <span class="op">=</span> pipeline(<span class="st">"text-classification"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="text-generation" class="level3">
<h3 class="anchored" data-anchor-id="text-generation">2. Text Generation</h3>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>text_generator <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="token-classification" class="level3">
<h3 class="anchored" data-anchor-id="token-classification">3. Token Classification</h3>
<p>Assigning labels to individual tokens in a sequence (e.g., Named Entity Recognition, Part-of-Speech Tagging):</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>token_classifier <span class="op">=</span> pipeline(<span class="st">"token-classification"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="question-answering" class="level3">
<h3 class="anchored" data-anchor-id="question-answering">4. Question Answering</h3>
<p>Extracting an answer from a given context based on a question:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>question_answerer <span class="op">=</span> pipeline(<span class="st">"question-answering"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="text-generation-1" class="level3">
<h3 class="anchored" data-anchor-id="text-generation-1">5. Text Generation</h3>
<p>Generating text based on a given prompt (e.g., Language Modeling, Story Generation):</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>text_generator <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="summarization" class="level3">
<h3 class="anchored" data-anchor-id="summarization">6. Summarization</h3>
<p>Condensing long documents into shorter summaries:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>summarizer <span class="op">=</span> pipeline(<span class="st">"summarization"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="translation" class="level3">
<h3 class="anchored" data-anchor-id="translation">7. Translation</h3>
<p>Translating text from one language to another:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>translator <span class="op">=</span> pipeline(<span class="st">"translation"</span>, model<span class="op">=</span><span class="st">"Helsinki-NLP/opus-mt-en-fr"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="text2text-generation" class="level3">
<h3 class="anchored" data-anchor-id="text2text-generation">8. Text2Text Generation</h3>
<p>General-purpose text transformation (e.g., summarization, translation):</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>text2text_generator <span class="op">=</span> pipeline(<span class="st">"text2text-generation"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="example-tasks-using-hugging-face-transformers-pipeline" class="level2">
<h2 class="anchored" data-anchor-id="example-tasks-using-hugging-face-transformers-pipeline">Example Tasks using Hugging Face <code>transformers</code> Pipeline</h2>
<section id="task-1-sentiment-analysis" class="level3">
<h3 class="anchored" data-anchor-id="task-1-sentiment-analysis">Task 1: Sentiment Analysis</h3>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>classifier <span class="op">=</span> pipeline(<span class="st">"sentiment-analysis"</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> classifier(<span class="st">"I was happy to visit my home country"</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="task-2-text-generation" class="level3">
<h3 class="anchored" data-anchor-id="task-2-text-generation">Task 2: Text Generation</h3>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>text_generator <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>, model<span class="op">=</span><span class="st">"distilbert/distilgpt2"</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> text_generator(</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Today is a rainy day in London"</span>,</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    num_return_sequences<span class="op">=</span><span class="dv">2</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated_text:</span><span class="ch">\n</span><span class="st">"</span>, generated_text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="task-3-question-answering" class="level3">
<h3 class="anchored" data-anchor-id="task-3-question-answering">Task 3: Question Answering</h3>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>qa_model <span class="op">=</span> pipeline(<span class="st">"question-answering"</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>question <span class="op">=</span> <span class="st">"What is my job?"</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>context <span class="op">=</span> <span class="st">"I am developing AI models with Python."</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(qa_model(question<span class="op">=</span>question, context<span class="op">=</span>context))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
</section>

</main>
<!-- /main column -->
<div class="custom-footer">

  © 2025 Arundev Vamadevan — All rights reserved.

</div>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>
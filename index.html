<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Arundev Vamadevan">

<title>Generative AI - Encoder-Decoder, Attention, and Transformers</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="index_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="index_files/libs/quarto-html/anchor.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="index_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="index_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="index_files/libs/bootstrap/bootstrap-bb462d781dde1847d9e3ccf7736099dd.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">


<link rel="stylesheet" href="footer.css">
<link rel="stylesheet" href="watermark.css">
</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Generative AI - Encoder-Decoder, Attention, and Transformers</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p><a href="https://www.linkedin.com/in/arundev-v">Arundev Vamadevan</a> </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="Images/frame.png" class="img-fluid figure-img" width="200"></p>
<figcaption>Scan to visit the Github Page</figcaption>
</figure>
</div>
<section id="generative-ai" class="level1">
<h1>Generative AI</h1>
<section id="encoder-decoder" class="level2">
<h2 class="anchored" data-anchor-id="encoder-decoder">Encoder &amp; Decoder</h2>
<p>So far, we have learned different RNN models, but they have different problems:</p>
<ol type="1">
<li><strong>Simple RNN</strong> → <em>Vanishing gradient problem</em></li>
<li><strong>LSTM-RNN</strong></li>
<li><strong>GRU-RNN</strong></li>
</ol>
<blockquote class="blockquote">
<p>LSTM (Long Short Term Memory) and GRU (Gated Reccurent Unit) have long short-term memory and are very efficient in solving problems like many-to-one RNN tasks (e.g., sentiment analysis, predicting the next word).</p>
</blockquote>
<hr>
</section>
<section id="lstm-vs-gru-comparison" class="level2">
<h2 class="anchored" data-anchor-id="lstm-vs-gru-comparison">LSTM vs GRU Comparison</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 82%">
</colgroup>
<thead>
<tr class="header">
<th>LSTM</th>
<th>GRU (Gated Recurrent Unit)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Uses a gating mechanism with 3 gates: input, forget, output</td>
<td>Has 2 gates: update and reset</td>
</tr>
<tr class="even">
<td>Excellent for long-term dependency</td>
<td>Performs similarly with fewer parameters</td>
</tr>
<tr class="odd">
<td>Maintains context over longer periods</td>
<td>More efficient and simpler</td>
</tr>
<tr class="even">
<td>Good for vanishing gradient issues</td>
<td>Good performance and efficiency</td>
</tr>
</tbody>
</table>
<hr>
</section>
<section id="limitation-of-lstm-and-gru---they-lacks-future-context" class="level2">
<h2 class="anchored" data-anchor-id="limitation-of-lstm-and-gru---they-lacks-future-context">Limitation of LSTM and GRU - They lacks future context</h2>
<section id="reason-unidirectional-processing" class="level4">
<h4 class="anchored" data-anchor-id="reason-unidirectional-processing">Reason : Unidirectional Processing</h4>
<p>LSTM &amp; GRU lack future context due to one-way processing. Both process sequence in only one direction - from left to right (or from past to future). ie when making a prediction at position t, they only have access to information from 0 to t, not from t+1 onwards.</p>
<p><strong>Example:</strong></p>
<blockquote class="blockquote">
<p>“The person who treats patients in the _____ is a doctor.”</p>
</blockquote>
<p>A standard LSTM / GRU processing the blank would not yet know that “doctor” is coming, which would be a strong clue that “hospital” should fill the blank.</p>
</section>
<section id="solution-bidirectional-rnns" class="level3">
<h3 class="anchored" data-anchor-id="solution-bidirectional-rnns">Solution: Bidirectional RNNs</h3>
<p>Works by running two separate RNNs (can be LSTM or GRU cells) simultaneously in opposite directions and their outputs are combined.</p>
</section>
</section>
<section id="bidirectional-rnn" class="level2">
<h2 class="anchored" data-anchor-id="bidirectional-rnn">Bidirectional RNN</h2>
<ul>
<li>B-RNN captures both past (forward RNN) and future (backward RNN) context.</li>
<li>Effective in translation, speech recognition, and language understanding tasks.</li>
<li>Downside: Double computational cost.</li>
<li>Not suited for real-time streaming applications.</li>
</ul>
<hr>
</section>
<section id="sequence-to-sequence-processing" class="level2">
<h2 class="anchored" data-anchor-id="sequence-to-sequence-processing">Sequence-to-Sequence Processing</h2>
<p>Example: <strong>Machine Translation (English to Hindi)</strong></p>
<p>Giving a sequence of inputs and producing a sequence of outputs. Type of RNN needed : Many to Many</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/1_Seq-To-Seq.png" class="img-fluid figure-img"></p>
<figcaption>Image - Seq2Seq Architecture</figcaption>
</figure>
</div>
<p>These kind of scenarios where sequence of inputs accepted and sequence of output should generated, Basic RNNs will struggle and will not give good results. That is why We need <strong>Encoder–Decoder Architecture</strong>.</p>
<p>Another example: chatbot<br>
&gt; Input: “Hi, how are you?” → Output: response is also a sequence.</p>
</section>
<section id="encoder-decoder-architecture" class="level2">
<h2 class="anchored" data-anchor-id="encoder-decoder-architecture">Encoder-Decoder Architecture</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/2_Enc-Dec-Arch.png" class="img-fluid figure-img"></p>
<figcaption>Image - Encoder - Decoder Architecture</figcaption>
</figure>
</div>
<section id="encoder-process" class="level3">
<h3 class="anchored" data-anchor-id="encoder-process">1. Encoder Process</h3>
<ol type="1">
<li>Processes input sequence one token at a time.</li>
<li>As it process each token, the hidden state is also updated to capture the information about the sequence.</li>
<li>After processing the entire sequences, Final hidden state = context vector.</li>
<li>This context vector represents the summarized representation of entire input sequence.</li>
</ol>
<hr>
</section>
<section id="context-vector-transfer" class="level3">
<h3 class="anchored" data-anchor-id="context-vector-transfer">2. Context Vector Transfer</h3>
<ul>
<li>Contains compressed representation of the input.</li>
<li>Bottleneck: condenses all info into one fixed-length vector.</li>
<li>Sent to decoder as initial hidden state to start the decoding process.</li>
</ul>
<hr>
</section>
<section id="decoder-process" class="level3">
<h3 class="anchored" data-anchor-id="decoder-process">3. Decoder Process</h3>
<ol type="1">
<li>Receives context vector as initial hidden state.</li>
<li>Outputs sequence (eg:- Translation) one token at a time.</li>
<li>At each step decoder uses current hidden state and previously generated token.</li>
</ol>
<p>Example : “Good Morning” → Translate to German Language</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/3_Enc-Dec-Example_1.png" class="img-fluid figure-img"></p>
<figcaption>Encoder - Decoder Example</figcaption>
</figure>
</div>
</section>
<section id="embedded-layer" class="level3">
<h3 class="anchored" data-anchor-id="embedded-layer">Embedded Layer</h3>
<ol type="1">
<li>Convert input sentence into tokens (via embedding layer or word2vec).</li>
<li>Transforms each token into a fixed-size dense vector.</li>
<li>These vectors capture word semantics.</li>
</ol>
<p>Example: Suppose we have an english vocabulary of 1000 words and each word is represented as a 300-dimensional vector. The words “Good” and “Morning” are each converted into 300 dimensional vector using embedding layer.These vectors capture meaning and relationships between words (semantics)</p>
<p>“Good” → [0.13, -0.72, 0.05, …, 0.91] (300 numbers)</p>
<p>“Morning” → [0.44, -0.11, 0.63, …, -0.21] (300 numbers)</p>
<p>300 dimension Used in Word2Vec (Google) and GloVe (Stanford) pretrained embeddings to balance performance vs computational cos, butit could be</p>
<ol type="1">
<li><p>50, 100, 200 (faster, lighter)</p></li>
<li><p>300 (balanced)</p></li>
<li><p>512 or 768 (used in BERT, GPT models)</p></li>
</ol>
<hr>
</section>
<section id="encoder" class="level3">
<h3 class="anchored" data-anchor-id="encoder">Encoder</h3>
<ul>
<li>LSTM processes input sequence.</li>
<li>Final state = context vector</li>
</ul>
</section>
<section id="decoder" class="level3">
<h3 class="anchored" data-anchor-id="decoder">Decoder</h3>
<ul>
<li>Another LSTM to generate translated output.</li>
<li>Uses context vector, previous token, and its own hidden state.</li>
</ul>
<hr>
</section>
<section id="output-layer-fully-connected-softmax" class="level3">
<h3 class="anchored" data-anchor-id="output-layer-fully-connected-softmax">Output Layer (Fully Connected Softmax)</h3>
<ul>
<li>Decoder output → Fully connected layer → Softmax : The output of LSTM at each step is passed though a fully connected dense layer to transform it into a vector of the same size as the German vocabulary.</li>
<li>Converts to probability distribution : The vector is passed through softmax function -&gt; turns into probability distribution between 0 and 1.</li>
<li>Highest value index = predicted word</li>
</ul>
</section>
</section>
<section id="limitation-of-encoderdecoder-architecture" class="level2">
<h2 class="anchored" data-anchor-id="limitation-of-encoderdecoder-architecture">Limitation of Encoder–Decoder Architecture</h2>
<ul>
<li>One fixed context vector (encoders final state) used for all outputs.</li>
<li>If sentence is long, compression leads to <strong>information loss</strong>.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/4_seq2seq.png" class="img-fluid figure-img"></p>
<figcaption>Bleu Score vs Sentence Length</figcaption>
</figure>
</div>
<p>Since it only consider the context vector in final state (not from each state), the context vector tend to have more information with nearest word which has seen in nearest time stamp. This problem is called <strong>Information Bottleneck</strong></p>
<section id="solution" class="level4">
<h4 class="anchored" data-anchor-id="solution">Solution</h4>
<p>Give equal importance to all words in input sequence regardless of relevance to the current output token.</p>
<ol type="1">
<li><p><strong>Encoder-Decoder with Attention Mechanism</strong><br>
Allows the decoder to focus on different parts of the input sequence at each decoding step.</p></li>
<li><p><strong>Transformer Architecture</strong><br>
Replaces traditional RNNs in encoder-decoder with a self-attention mechanism to better capture relationships between all words in the sequence.</p></li>
</ol>
<p>Therefore, with attention mechanism, the final one-time context vector transfer is replaced with a <strong>dynamic attention process</strong>, enabling the decoder to access different parts of the encoder output at each generation step.</p>
</section>
</section>
<section id="attention-based-encoder-decoder" class="level2">
<h2 class="anchored" data-anchor-id="attention-based-encoder-decoder">Attention-Based Encoder-Decoder</h2>
<ol type="1">
<li><strong>Encoder Output</strong>
<ul>
<li>Keep all hidden states<br>
</li>
</ul></li>
<li><strong>Alignment Score</strong>
<ul>
<li>Score between current decoder state and each encoder state<br>
</li>
</ul></li>
<li><strong>Attention Weights</strong>
<ul>
<li>Use softmax to compute probabilities<br>
</li>
</ul></li>
<li><strong>Context Vector</strong>
<ul>
<li>Weighted sum of encoder states<br>
</li>
</ul></li>
<li><strong>Output Generation</strong>
<ul>
<li>Combines context vector + previous output + decoder state</li>
</ul></li>
</ol>
<hr>
</section>
<section id="limitations-of-attention" class="level2">
<h2 class="anchored" data-anchor-id="limitations-of-attention">Limitations of Attention</h2>
<ol type="1">
<li><strong>Sequential Processing</strong> – No parallelization<br>
</li>
<li><strong>Long-Range Dependency</strong> – Still struggles with very long sequences<br>
</li>
<li><strong>Vanishing/Exploding Gradients</strong><br>
</li>
<li><strong>Computational Cost</strong></li>
</ol>
<hr>
</section>
<section id="transformers" class="level2">
<h2 class="anchored" data-anchor-id="transformers">Transformers</h2>
<section id="why-transformers" class="level3">
<h3 class="anchored" data-anchor-id="why-transformers">Why Transformers?</h3>
<ol type="1">
<li><strong>Parallelization</strong>
<ul>
<li>Processes whole sequence at once<br>
</li>
<li>Faster on GPU/TPU<br>
</li>
</ul></li>
<li><strong>Direct Connection</strong>
<ul>
<li>Self-attention allows connections between all positions<br>
</li>
<li>Solves information highway problem<br>
</li>
</ul></li>
<li><strong>Multi-head Attention</strong>
<ul>
<li>Attend to different representations<br>
</li>
<li>Capture syntax, semantics, relationships<br>
</li>
<li>Enables parallel computation</li>
</ul></li>
</ol>
<pre class="text"><code>Sentence: “The athlete who won many medals stopped competing”
- Head 1: “athlete” → “stopped” (subject – verb)
- Head 2: “athlete” → “won” (relative clause)
- Head 3: “won” → “medals” (object)</code></pre>
<hr>
<ol start="4" type="1">
<li><strong>Positional Encoding</strong>
<ul>
<li>Adds unique vectors to word embeddings for word order awareness</li>
</ul></li>
</ol>
<pre class="text"><code>"Police arrested protester" ≠ "Protester arrested police"</code></pre>
<ol start="5" type="1">
<li><strong>Scalability</strong>
<ul>
<li>Efficient for large models and datasets</li>
</ul></li>
</ol>
<hr>
</section>
</section>
<section id="context-vectors-in-transformers" class="level2">
<h2 class="anchored" data-anchor-id="context-vectors-in-transformers">Context Vectors in Transformers</h2>
<p>Contextualized representations of tokens using self-attention mechanism.</p>
<p>Steps:<br>
1. Input Embedding<br>
2. Positional Encoding<br>
3. Self-Attention<br>
4. Feedforward Layers<br>
5. Contextualized Output</p>
<hr>
</section>
<section id="contextual-embeddings-vs-static-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="contextual-embeddings-vs-static-embeddings">Contextual Embeddings vs Static Embeddings</h2>
<p><strong>Word2Vec (Static):</strong><br>
&gt; Same vector for word in all contexts<br>
→ “Light the candle” vs “Sun gives light”</p>
<p><strong>Contextual Embedding:</strong><br>
- Varies depending on sentence</p>
<p><strong>Example:</strong><br>
The person who treats people is a __________?</p>
<hr>
</section>
<section id="transformer-architecture" class="level2">
<h2 class="anchored" data-anchor-id="transformer-architecture">Transformer Architecture</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/5_transformer.png" class="img-fluid figure-img"></p>
<figcaption>Tranformer Architecture</figcaption>
</figure>
</div>
<section id="encoder-1" class="level3">
<h3 class="anchored" data-anchor-id="encoder-1">Encoder:</h3>
<ol type="1">
<li>Input Embedding + Positional Encoding<br>
</li>
<li>Self-Attention<br>
</li>
<li>Feedforward Layers<br>
</li>
<li>Encoder Output</li>
</ol>
</section>
<section id="decoder-1" class="level3">
<h3 class="anchored" data-anchor-id="decoder-1">Decoder:</h3>
<ol type="1">
<li>Output Embedding + Positional Encoding<br>
</li>
<li>Masked Self-Attention<br>
</li>
<li>Cross-Attention with Encoder Output<br>
</li>
<li>Feedforward Layers<br>
</li>
<li>Output Projection (Softmax)</li>
</ol>
<hr>
</section>
</section>
<section id="bert-vs-gpt" class="level2">
<h2 class="anchored" data-anchor-id="bert-vs-gpt">BERT vs GPT</h2>
<ul>
<li><strong>BERT</strong> – Encoder only (e.g., QA, NER)<br>
</li>
<li><strong>GPT</strong> – Decoder only (text generation)</li>
</ul>
<hr>
</section>
<section id="what-is-generative-ai" class="level2">
<h2 class="anchored" data-anchor-id="what-is-generative-ai">What is Generative AI?</h2>
<ul>
<li>Creates original content: text, image, music, code, video<br>
</li>
<li>Uses learned patterns to generate new, realistic outputs</li>
</ul>
<p><strong>Example:</strong> DALL·E – Generates new image from prompt</p>
<hr>
</section>
<section id="types-of-generative-ai-models" class="level2">
<h2 class="anchored" data-anchor-id="types-of-generative-ai-models">Types of Generative AI Models</h2>
<ol type="1">
<li>Generative Adversarial Networks (GANs)<br>
</li>
<li>Variational Autoencoders (VAEs)<br>
</li>
<li>Transformer Models (e.g., GPT)</li>
</ol>
<hr>
<section id="gans" class="level3">
<h3 class="anchored" data-anchor-id="gans">GANs</h3>
<ul>
<li>Image generation<br>
</li>
<li>Generator vs Discriminator network<br>
</li>
<li>Generator improves over time</li>
</ul>
<hr>
</section>
<section id="vaes" class="level3">
<h3 class="anchored" data-anchor-id="vaes">VAEs</h3>
<ul>
<li>Encodes input to latent space<br>
</li>
<li>Reconstructs or generates new data<br>
</li>
<li>Use case: reconstructing medical images</li>
</ul>
<hr>
</section>
<section id="transformers-1" class="level3">
<h3 class="anchored" data-anchor-id="transformers-1">Transformers</h3>
<ul>
<li>Effective in text generation<br>
</li>
<li>Self-attention for context<br>
</li>
<li>Scalable to large sequences and datasets</li>
</ul>
<hr>
</section>
</section>
<section id="large-language-models-llms" class="level2">
<h2 class="anchored" data-anchor-id="large-language-models-llms">Large Language Models (LLMs)</h2>
<ul>
<li>Trained on massive datasets<br>
</li>
<li>Understand and generate human language<br>
</li>
<li>Billions of parameters (e.g., GPT-3 = 175B)</li>
</ul>
<p><strong>Use Cases:</strong><br>
Chatbots, Q&amp;A, summarization, speech recognition</p>
<hr>
</section>
<section id="how-llms-work" class="level2">
<h2 class="anchored" data-anchor-id="how-llms-work">How LLMs Work</h2>
<section id="transformer-architecture-1" class="level3">
<h3 class="anchored" data-anchor-id="transformer-architecture-1">Transformer Architecture</h3>
<ul>
<li>Uses self-attention<br>
</li>
<li>Generates token-by-token</li>
</ul>
</section>
<section id="self-supervised-learning" class="level3">
<h3 class="anchored" data-anchor-id="self-supervised-learning">Self-Supervised Learning</h3>
<ul>
<li>Predicts next word without labels</li>
</ul>
</section>
<section id="tokenization" class="level3">
<h3 class="anchored" data-anchor-id="tokenization">Tokenization</h3>
<ul>
<li>Breaks text into tokens<br>
</li>
<li>Token limit includes input + output</li>
</ul>
<hr>
</section>
</section>
<section id="llm-providers" class="level2">
<h2 class="anchored" data-anchor-id="llm-providers">LLM Providers</h2>
<ul>
<li><strong>OpenAI</strong> – GPT-3/4, DALL·E, Whisper<br>
</li>
<li><strong>Anthropic</strong> – Claude<br>
</li>
<li><strong>Meta AI</strong> – LLaMA<br>
</li>
<li><strong>Hugging Face</strong> – BERT, T5, GPT-2<br>
</li>
<li><strong>Microsoft Azure</strong>, <strong>Google Vertex</strong>, <strong>Amazon Titan</strong></li>
</ul>
<hr>
</section>
<section id="choosing-the-right-model" class="level2">
<h2 class="anchored" data-anchor-id="choosing-the-right-model">Choosing the Right Model</h2>
<ul>
<li>Quality<br>
</li>
<li>Speed<br>
</li>
<li>Price<br>
</li>
<li>Latency</li>
</ul>
<hr>
</section>
<section id="open-source-vs-proprietary" class="level2">
<h2 class="anchored" data-anchor-id="open-source-vs-proprietary">Open Source vs Proprietary</h2>
<section id="open-source" class="level3">
<h3 class="anchored" data-anchor-id="open-source">Open Source</h3>
<ul>
<li>Free and modifiable<br>
</li>
<li>Examples: LLaMA, GPT-Neo</li>
</ul>
</section>
<section id="proprietary" class="level3">
<h3 class="anchored" data-anchor-id="proprietary">Proprietary</h3>
<ul>
<li>API only<br>
</li>
<li>Examples: GPT-4, Claude</li>
</ul>
<hr>
</section>
</section>
<section id="hugging-face-and-ollama" class="level2">
<h2 class="anchored" data-anchor-id="hugging-face-and-ollama">HUGGING FACE AND Ollama</h2>
<p>Hugging Face and Ollama serve different purposes in the LLM ecosystem:</p>
<section id="hugging-face" class="level3">
<h3 class="anchored" data-anchor-id="hugging-face">Hugging Face</h3>
<ul>
<li>A comprehensive AI platform and community hub<br>
</li>
<li>Hosts thousands of open-source models across various AI domains (not just LLMs)<br>
</li>
<li>Provides tools for model training, fine-tuning, and deployment<br>
</li>
<li>Offers both free and paid services (Inference API)<br>
</li>
<li>Includes datasets, documentation, and research papers<br>
</li>
<li>A hosting platform where users can create and share machine learning web applications with minimal setup.<br>
</li>
<li>Has enterprise-focused solutions<br>
</li>
<li>Web-based interface with extensive collaboration features<br>
</li>
<li>Focuses on being a complete MLOps platform with model cards, versioning, etc.</li>
</ul>
</section>
<section id="ollama" class="level3">
<h3 class="anchored" data-anchor-id="ollama">Ollama</h3>
<ul>
<li>Focused specifically on running LLMs locally<br>
</li>
<li>Simplified command-line interface for downloading and running open-source models<br>
</li>
<li>Prioritizes ease of use for personal computing environments<br>
</li>
<li>Optimized for desktop/laptop performance with efficient resource usage<br>
</li>
<li>Provides a simple API for local application integration<br>
</li>
<li>Includes model library management functionality<br>
</li>
<li>Emphasizes privacy by keeping all processing on your machine<br>
</li>
<li>Primarily targets individual developers and hobbyists</li>
</ul>
<p><strong>The main difference is scope and focus</strong>:<br>
Hugging Face is a comprehensive AI platform with broad capabilities, while Ollama is a specialized tool designed specifically to make running LLMs locally as simple as possible.</p>
<hr>
</section>
<section id="ollama-capabilities" class="level3">
<h3 class="anchored" data-anchor-id="ollama-capabilities">Ollama Capabilities</h3>
<ol type="1">
<li>Generate text<br>
</li>
<li>Answer a question<br>
</li>
<li>Summarize text<br>
</li>
<li>Translate text</li>
</ol>
<hr>
</section>
</section>
<section id="hugging-face-1" class="level2">
<h2 class="anchored" data-anchor-id="hugging-face-1">HUGGING FACE</h2>
<p>Hugging Face is a platform in which we can do Gen AI projects, NLP projects, and Computer Vision projects.<br>
Here we are more focusing on Gen AI tasks.</p>
<p>Hugging Face has lots of Large Language Models and Datasets that can be used to train these kinds of LLMs.<br>
Spaces – provides infrastructure for training models and deploy, but it is a paid service.</p>
<p>Hugging Face provides one pipeline, so with the help of a pipeline we can easily perform any kind of task.<br>
Hugging Face also has one Python library called <strong>Transformers</strong>, which can be used for creating any kind of Gen AI project.</p>
<p>We will use a library called <code>Transformers</code>.<br>
<strong>Transformer</strong> is the architecture but <strong>Transformers</strong> is the library.<br>
It is named because it has all the models which are made based on Transformer architecture.</p>
<p>If we want to use the Hugging Face platform with the help of Python, we need the <strong>Transformers</strong> library (It has all the functionalities to use it with Python).</p>
<section id="pipeline-function-used" class="level4">
<h4 class="anchored" data-anchor-id="pipeline-function-used">Pipeline – function used</h4>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="nlp-tasks" class="level2">
<h2 class="anchored" data-anchor-id="nlp-tasks">NLP Tasks</h2>
<section id="text-classification" class="level3">
<h3 class="anchored" data-anchor-id="text-classification">1. Text Classification</h3>
<p>Assigning a category to a piece of text (e.g., Sentiment Analysis, Topic Classification, Spam Detection):</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>classifier <span class="op">=</span> pipeline(<span class="st">"text-classification"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="text-generation" class="level3">
<h3 class="anchored" data-anchor-id="text-generation">2. Text Generation</h3>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>text_generator <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="token-classification" class="level3">
<h3 class="anchored" data-anchor-id="token-classification">3. Token Classification</h3>
<p>Assigning labels to individual tokens in a sequence (e.g., Named Entity Recognition, Part-of-Speech Tagging):</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>token_classifier <span class="op">=</span> pipeline(<span class="st">"token-classification"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="question-answering" class="level3">
<h3 class="anchored" data-anchor-id="question-answering">4. Question Answering</h3>
<p>Extracting an answer from a given context based on a question:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>question_answerer <span class="op">=</span> pipeline(<span class="st">"question-answering"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="text-generation-1" class="level3">
<h3 class="anchored" data-anchor-id="text-generation-1">5. Text Generation</h3>
<p>Generating text based on a given prompt (e.g., Language Modeling, Story Generation):</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>text_generator <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="summarization" class="level3">
<h3 class="anchored" data-anchor-id="summarization">6. Summarization</h3>
<p>Condensing long documents into shorter summaries:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>summarizer <span class="op">=</span> pipeline(<span class="st">"summarization"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="translation" class="level3">
<h3 class="anchored" data-anchor-id="translation">7. Translation</h3>
<p>Translating text from one language to another:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>translator <span class="op">=</span> pipeline(<span class="st">"translation"</span>, model<span class="op">=</span><span class="st">"Helsinki-NLP/opus-mt-en-fr"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="text2text-generation" class="level3">
<h3 class="anchored" data-anchor-id="text2text-generation">8. Text2Text Generation</h3>
<p>General-purpose text transformation (e.g., summarization, translation):</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>text2text_generator <span class="op">=</span> pipeline(<span class="st">"text2text-generation"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="example-tasks-using-hugging-face-transformers-pipeline" class="level2">
<h2 class="anchored" data-anchor-id="example-tasks-using-hugging-face-transformers-pipeline">Example Tasks using Hugging Face <code>transformers</code> Pipeline</h2>
<section id="task-1-sentiment-analysis" class="level3">
<h3 class="anchored" data-anchor-id="task-1-sentiment-analysis">Task 1: Sentiment Analysis</h3>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>classifier <span class="op">=</span> pipeline(<span class="st">"sentiment-analysis"</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> classifier(<span class="st">"I was happy to visit my home country"</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="task-2-text-generation" class="level3">
<h3 class="anchored" data-anchor-id="task-2-text-generation">Task 2: Text Generation</h3>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>text_generator <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>, model<span class="op">=</span><span class="st">"distilbert/distilgpt2"</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> text_generator(</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Today is a rainy day in London"</span>,</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    num_return_sequences<span class="op">=</span><span class="dv">2</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Generated_text:</span><span class="ch">\n</span><span class="st">"</span>, generated_text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="task-3-question-answering" class="level3">
<h3 class="anchored" data-anchor-id="task-3-question-answering">Task 3: Question Answering</h3>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>qa_model <span class="op">=</span> pipeline(<span class="st">"question-answering"</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>question <span class="op">=</span> <span class="st">"What is my job?"</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>context <span class="op">=</span> <span class="st">"I am developing AI models with Python."</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(qa_model(question<span class="op">=</span>question, context<span class="op">=</span>context))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="llm-augmentation" class="level2">
<h2 class="anchored" data-anchor-id="llm-augmentation">LLM Augmentation:</h2>
<p>LLM (Large Language Model) Augmentation refers to techniques used to enhance the capabilities of large language models by integrating external data sources, retrieval mechanisms, fine-tuning strategies, or computational methods to improve their accuracy, relevance, and contextual understanding.</p>
<section id="techniques-of-llm-augmentation" class="level3">
<h3 class="anchored" data-anchor-id="techniques-of-llm-augmentation">Techniques of LLM Augmentation</h3>
<ol type="1">
<li><strong>Retrieval-Augmented Generation (RAG)</strong>
<ul>
<li>Combines LLMs with an external knowledge base to fetch relevant information before generating a response.</li>
</ul></li>
<li><strong>Fine-Tuning &amp; Parameter Efficient Fine-Tuning (PEFT)</strong><br>
Fine-tuning the model using domain-specific datasets to improve its performance in a particular task.<br>
Techniques:
<ul>
<li>Full fine-tuning<br>
</li>
<li>LoRA (Low-Rank Adaptation)<br>
</li>
<li>QLoRA (Quantized LoRA)<br>
</li>
<li>Prefix Tuning</li>
</ul></li>
<li><strong>Prompt Engineering &amp; Prompt Tuning</strong><br>
Optimizing input prompts to get more accurate and relevant responses from the model.<br>
Techniques:
<ul>
<li>Zero-shot, Few-shot, and Chain-of-Thought (CoT) prompting<br>
</li>
<li>Self-consistency prompting<br>
</li>
<li>Prompt tuning with soft tokens</li>
</ul></li>
<li><strong>Knowledge Injection &amp; External APIs</strong><br>
Integrating real-time or domain-specific knowledge via APIs or databases.<br>
Techniques:
<ul>
<li>SQL-based data augmentation<br>
</li>
<li>Graph-based knowledge retrieval<br>
</li>
<li>API calls to real-time sources</li>
</ul></li>
<li><strong>Memory-Augmented LLMs</strong><br>
Allowing LLMs to retain contextual knowledge over multiple interactions.<br>
Techniques:
<ul>
<li>Persistent memory (e.g., LangChain memory modules)<br>
</li>
<li>Context caching and dynamic context expansion</li>
</ul></li>
<li><strong>Multimodal Augmentation</strong><br>
Extending LLMs to process text along with other modalities like images, videos, and speech.<br>
Techniques:
<ul>
<li>Vision-Language Models (e.g., BLIP, Flamingo)<br>
</li>
<li>Audio-Language Fusion (e.g., Whisper + LLM)</li>
</ul></li>
<li><strong>Self-Refinement &amp; Reinforcement Learning</strong><br>
LLMs refine their own responses using reinforcement mechanisms.<br>
Techniques:
<ul>
<li>Reinforcement Learning from Human Feedback (RLHF)<br>
</li>
<li>Self-consistency and self-distillation</li>
</ul></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/6_Prompt_Engineering.png" class="img-fluid figure-img"></p>
<figcaption>Prompt Engineering</figcaption>
</figure>
</div>
</section>
</section>
<section id="prompt" class="level2">
<h2 class="anchored" data-anchor-id="prompt">Prompt</h2>
<p>The input or instruction we give to an LLM to guide its response. It could be question, a command or even just a phrase. The way we phrase our prompt plays a huge role in how the model responds. Prompts is like a set of directions that we give to the LLM to do tasks like Answer a question, summarize text, generate a story or even providing code snippets.</p>
<section id="understanding-a-prompt" class="level3">
<h3 class="anchored" data-anchor-id="understanding-a-prompt">Understanding A Prompt</h3>
<p>We can think of prompts as programs for LLMs. Just like how a traditional program gives instructions to a computer to perform a specific task. Prompt tells the LLM how to respond based on our input and is designed to get the model to produce responses that meet your specific requirements. The better the prompt, the better the models response. So, we can tell promps as the foundation of the interaction with the LLM. Its structure and clarity directly influence the quality of the response.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/7_Prompt.png" class="img-fluid figure-img"></p>
<figcaption>Prompt Example</figcaption>
</figure>
</div>
</section>
</section>
<section id="elements-of-a-prompt" class="level2">
<h2 class="anchored" data-anchor-id="elements-of-a-prompt">Elements of a prompt</h2>
<section id="instruction-or-task" class="level3">
<h3 class="anchored" data-anchor-id="instruction-or-task">Instruction or Task</h3>
<p>The core request for the model, a specific task or instruction we want the model to perform.</p>
</section>
<section id="context" class="level3">
<h3 class="anchored" data-anchor-id="context">Context</h3>
<p>Background information to guide the response, like external information or additional context</p>
</section>
<section id="input-data" class="level3">
<h3 class="anchored" data-anchor-id="input-data">Input Data</h3>
<p>The input or question that we are interested to find a response for</p>
</section>
<section id="output-format" class="level3">
<h3 class="anchored" data-anchor-id="output-format">Output Format</h3>
<p>Defines how the response should be structured, the type or format of the output.</p>
<hr>
</section>
<section id="examples---elements-of-a-prompt" class="level3">
<h3 class="anchored" data-anchor-id="examples---elements-of-a-prompt">Examples - Elements of a Prompt</h3>
<p><strong>Instruction or Task</strong><br>
You will be provided with statements, and your task is to convert them to standard English</p>
<p><strong>Context</strong><br>
You are a customer service representative. Explain the return policy to a customer who is unhappy with their purchase</p>
<p><strong>Input Data</strong><br>
The customer purchased a smart speaker but reported that it is not connecting to their<br>
Wi-Fi. They have tried restarting both the router and the speaker, but the issue persists</p>
<p><strong>Output Format</strong><br>
Provide a list of the top 5 features of this product<br>
Write a 2-paragraph summary of the article<br>
Create a table comparing solar energy</p>
<hr>
</section>
</section>
<section id="what-is-prompt-engineering" class="level2">
<h2 class="anchored" data-anchor-id="what-is-prompt-engineering">What is Prompt Engineering</h2>
<p>Crafting and refining prompts to improve model responses. Kind of thinking strategically to guide the model in the right direction. This involves experimenting and iterating with different wording, structure and styles.<br>
The first prompt we use wont give the best result. So refine it through iteration to optimize the response by maximizing the performance of the model and get answers that meet our specific needs.</p>
</section>
<section id="why-prompt-engineering-is-important" class="level2">
<h2 class="anchored" data-anchor-id="why-prompt-engineering-is-important">Why Prompt Engineering is important</h2>
<ul>
<li>Helps to get more accurate and relevant responses and allows us to shape the models behaviour.<br>
</li>
<li>We can design prompts to respond in a specific tone such as formal, friendly or technical. It also reduces vague or incorrect outputs by clarifying the instructions.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/8_steps.png" class="img-fluid figure-img"></p>
<figcaption>Templatized Prompt</figcaption>
</figure>
</div>
<section id="templatize" class="level4">
<h4 class="anchored" data-anchor-id="templatize">Templatize</h4>
<p>Create a reusable structure for prompts that can be applied to similar tasks<br>
Useful when doing repetitive tasks like summarize articles, generate reports, answer questions</p>
<hr>
</section>
</section>
<section id="advanced-prompting-techniques" class="level2">
<h2 class="anchored" data-anchor-id="advanced-prompting-techniques">Advanced Prompting Techniques</h2>
<p>Fine-tune interactions for more accurate, meaningful and context-aware outputs.<br>
Common prompting techniques:</p>
<ol type="1">
<li>Zero-shot, one-shot and few-shot Prompting<br>
</li>
<li>Chain-of-Thought Prompting<br>
</li>
<li>Role-based and Instruction-based prompting<br>
</li>
<li>Contextual Prompting with RAG</li>
</ol>
<hr>
<section id="zero-shot-prompting" class="level3">
<h3 class="anchored" data-anchor-id="zero-shot-prompting">Zero-Shot Prompting</h3>
<ul>
<li>Simplest and most widely used prompting technique<br>
</li>
<li>Provide the model with no prior examples or context before asking it to complete a task<br>
</li>
<li>The model must rely entirely on its pre-trained knowledge<br>
</li>
<li>Ask to complete task or answer question without offering any specific guidance or examples</li>
</ul>
<p><strong>Example:</strong><br>
<em>What is the capital of Japan?</em> — Tokyo (based on its pre-existing knowledge)</p>
<p>Ideal for simple, factual queries or tasks where the model doesn’t need any further context or instructions</p>
<hr>
</section>
<section id="one-shot-prompting" class="level3">
<h3 class="anchored" data-anchor-id="one-shot-prompting">One-Shot Prompting</h3>
<ul>
<li>Provide one example before asking the model to generate a response<br>
</li>
<li>Gives the model a bit of direction by showing it a single instance of what you want<br>
</li>
<li>Single example to follow, provide template or guide for its response<br>
</li>
<li>Ideal for the model to adopt a specific format, tone, or structure</li>
</ul>
<p><strong>Example:</strong><br>
First show it one product description as an example, then ask it to write a new one for a different product</p>
<hr>
</section>
<section id="few-shot-prompting" class="level3">
<h3 class="anchored" data-anchor-id="few-shot-prompting">Few-Shot Prompting</h3>
<ul>
<li>Provide a few examples of the task you want the model to perform before asking it<br>
</li>
<li>Gives the model clearer guidance and helps it understand patterns, structure, and tone<br>
</li>
<li>Produce a response that aligns expectations, useful for more complex or structured tasks<br>
</li>
<li>Provide the model with multiple examples to generate a similar response</li>
</ul>
<p><strong>Example:</strong><br>
Giving two or three sample summaries helps the model understand the format and what kind of details to include</p>
<hr>
</section>
<section id="chain-of-thought-prompting" class="level3">
<h3 class="anchored" data-anchor-id="chain-of-thought-prompting">Chain-of-Thought Prompting</h3>
<ul>
<li>Break down its thought process and provide a step-by-step explanation<br>
</li>
<li>Useful for logical reasoning, problem-solving or multi-step processes<br>
</li>
<li>Don’t just ask for a final answer — you ask the model to explain its reasoning in a step-by-step process</li>
</ul>
<p><strong>Example:</strong><br>
Solve a math problem, it will break down the calculation step by step</p>
<hr>
</section>
<section id="instruction-based-prompting" class="level3">
<h3 class="anchored" data-anchor-id="instruction-based-prompting">Instruction-based Prompting</h3>
<ul>
<li>Give the model very clear and direct instructions<br>
</li>
<li>Telling it exactly what you want it to do<br>
</li>
<li>Useful when you need specific results, like a summary, list, or report</li>
</ul>
<p><strong>Example:</strong><br>
<em>Write a 200-word summary of this article, then list three key points in bullet form</em><br>
The model follows your exact guidelines, producing structured and organized results</p>
</section>
</section>
<section id="choosing-the-right-optimization-prompt-engineering-rag-and-fine-tuning" class="level2">
<h2 class="anchored" data-anchor-id="choosing-the-right-optimization-prompt-engineering-rag-and-fine-tuning">Choosing the right optimization – Prompt Engineering, RAG and Fine-Tuning</h2>
<section id="comparison-prompt-engineering-vs-rag-vs-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="comparison-prompt-engineering-vs-rag-vs-fine-tuning">Comparison: Prompt Engineering vs RAG vs Fine-Tuning</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 32%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Prompt Engineering</strong></th>
<th><strong>RAG (Retrieval-Augmented Generation)</strong></th>
<th><strong>Fine-Tuning</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>No additional training required</td>
<td>Brings real-time, external data into the model</td>
<td>Adapt the model to domain-specific language</td>
</tr>
<tr class="even">
<td>Works out of the box</td>
<td>Great for dynamic content or live data queries</td>
<td>Lower latency and token usage</td>
</tr>
<tr class="odd">
<td>Easy to iterate and adjust</td>
<td>Reduces hallucination by retrieving documents</td>
<td>Best for well-defined, repeated use cases</td>
</tr>
<tr class="even">
<td>Limited by model’s original knowledge</td>
<td>Requires infrastructure (vector stores, indexing)</td>
<td>Requires a large dataset and compute resources</td>
</tr>
<tr class="odd">
<td>Long prompts can increase costs (more tokens)</td>
<td>Slower response time due to retrieval steps</td>
<td>Fine-tuned models may lose generalization</td>
</tr>
<tr class="even">
<td>Can lead to inconsistency without enough examples</td>
<td>Higher complexity to set up</td>
<td>Time-consuming to update when new data is available</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="technique-comparison-of-3-techniques" class="level2">
<h2 class="anchored" data-anchor-id="technique-comparison-of-3-techniques">Technique Comparison of 3 Techniques</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 27%">
<col style="width: 27%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Aspect</strong></th>
<th><strong>Prompt Engineering</strong></th>
<th><strong>RAG (Retrieval-Augmented Generation)</strong></th>
<th><strong>Fine-Tuning</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Setup Complexity</td>
<td>Low</td>
<td>Medium (requires vector store setup)</td>
<td>High (requires dataset and compute resources)</td>
</tr>
<tr class="even">
<td>Customization Level</td>
<td>Limited</td>
<td>High (dynamic retrieval)</td>
<td>Very High (specialized model behavior)</td>
</tr>
<tr class="odd">
<td>Response Time</td>
<td>Fast</td>
<td>Slower (retrieval adds latency)</td>
<td>Fast</td>
</tr>
<tr class="even">
<td>Maintenance Requirement</td>
<td>Low</td>
<td>Medium (update vector stores)</td>
<td>High (re-train for new use cases)</td>
</tr>
<tr class="odd">
<td>Use Case Examples</td>
<td>Quick answers, chatbots</td>
<td>Real-time data, semantic search</td>
<td>Specialized apps (finance, legal, healthcare)</td>
</tr>
<tr class="even">
<td>Cost</td>
<td>High (long prompts = more tokens)</td>
<td>Moderate (retrieval infrastructure costs)</td>
<td>High (training and infrastructure costs)</td>
</tr>
</tbody>
</table>
</section>
<section id="choosing-the-right-optimization" class="level2">
<h2 class="anchored" data-anchor-id="choosing-the-right-optimization">Choosing the right optimization</h2>
<section id="time-is-a-constraint" class="level3">
<h3 class="anchored" data-anchor-id="time-is-a-constraint">Time is a Constraint</h3>
<p>Start with Prompt Engineering — it’s fast, easy to experiment with, and requires no model changes.</p>
</section>
<section id="if-you-need-real-time-data" class="level3">
<h3 class="anchored" data-anchor-id="if-you-need-real-time-data">If You Need Real-Time Data</h3>
<p>Use RAG to fetch external or frequently updated information, ideal for apps like customer support, semantic search, or financial dashboards.</p>
</section>
<section id="if-you-need-high-accuracy-and-efficiency" class="level3">
<h3 class="anchored" data-anchor-id="if-you-need-high-accuracy-and-efficiency">If You Need High Accuracy and Efficiency</h3>
<p>Fine-tune the model when you want to reduce latency and token costs for well-defined, repetitive tasks like legal compliance checks or automated reporting.</p>
<hr>
</section>
<section id="best-practice" class="level3">
<h3 class="anchored" data-anchor-id="best-practice">Best Practice</h3>
<p>The best practice is to combine all three techniques to get the most out of your LLM.</p>
<ul>
<li>Prompt Engineering helps you prototype and test prompts quickly<br>
</li>
<li>RAG ensures that your model has access to the latest external information<br>
</li>
<li>Fine-Tuning makes the model more efficient for domain-specific tasks by reducing token usage and improving accuracy</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/9_Comparison.png" class="img-fluid figure-img"></p>
<figcaption>Best Practice</figcaption>
</figure>
</div>
<p><a href="https://platform.openai.com/docs/guides/optimizing-llm-accuracy">Source : Optimizing LLM Accuracy – OpenAI Documentation</a></p>
</section>
</section>
<section id="training-own-model-for-llm-optimization" class="level2">
<h2 class="anchored" data-anchor-id="training-own-model-for-llm-optimization">Training own model for LLM optimization</h2>
<section id="why-train-from-scratch" class="level3">
<h3 class="anchored" data-anchor-id="why-train-from-scratch">Why Train from Scratch?</h3>
<ul>
<li>If you need deep, specific domain knowledge without any bias from pre-existing models<br>
</li>
<li>If your data or the use case is unique, this approach provides more precise and controlled outcomes<br>
</li>
<li>In sensitive industries, having a model trained entirely on internal data can help meet stringent compliance requirements</li>
</ul>
<hr>
</section>
<section id="drawbacks-of-training-from-scratch" class="level3">
<h3 class="anchored" data-anchor-id="drawbacks-of-training-from-scratch">Drawbacks of Training from Scratch</h3>
<ul>
<li>Requires substantial computational power, storage, and sometimes weeks to complete<br>
</li>
<li>High-quality and large-scale data is essential, which can be costly and time-consuming to collect<br>
</li>
<li>Need specialized skills in deep learning, hyperparameter tuning, and model optimization<br>
</li>
<li>This process is lengthy compared to fine-tuning or prompt engineering</li>
</ul>
</section>
</section>
<section id="fine-tuning-llms" class="level2">
<h2 class="anchored" data-anchor-id="fine-tuning-llms">Fine-tuning LLMs</h2>
<p>Fine-tuning is the process of taking a pre-trained machine learning model and further training it on a specific dataset to adapt it for a particular task or domain. The model is already trained on large amount of data but needs to be optimized for domain-specific knowledge.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/10_fine_tuning.png" class="img-fluid figure-img"></p>
<figcaption>Fine Tuning</figcaption>
</figure>
</div>
<p>Fine-tuning is like giving the doctor specialized training in cardiology.<br>
After fine-tuning, the doctor retains their general medical knowledge but is now an expert in heart-related issues.</p>
</section>
<section id="why-fine-tune-a-pre-trained-model" class="level2">
<h2 class="anchored" data-anchor-id="why-fine-tune-a-pre-trained-model">Why Fine-Tune a Pre-Trained Model?</h2>
<p>Pre-trained language models, such as GPT variants, are built on large-scale, general-purpose datasets drawn from a wide range of internet sources. While these models exhibit strong general language understanding, they often lack the precision required for specific domains like healthcare, law, finance, or customer support.</p>
<hr>
<section id="key-benefits-of-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="key-benefits-of-fine-tuning">Key Benefits of Fine-Tuning</h3>
<section id="domain-adaptation" class="level4">
<h4 class="anchored" data-anchor-id="domain-adaptation"><strong>Domain Adaptation</strong></h4>
<p>Pre-trained models do not inherently understand proprietary information or specialized terminology.<br>
For instance, they won’t recognize your company’s latest product features unless fine-tuned on relevant documentation and data.</p>
</section>
<section id="enhanced-performance-in-niche-tasks" class="level4">
<h4 class="anchored" data-anchor-id="enhanced-performance-in-niche-tasks"><strong>Enhanced Performance in Niche Tasks</strong></h4>
<p>Fine-tuned models can handle domain-specific tasks like legal document review, medical record summarization, or technical support automation more accurately.</p>
</section>
<section id="cost-optimization" class="level4">
<h4 class="anchored" data-anchor-id="cost-optimization"><strong>Cost Optimization</strong></h4>
<p>By fine-tuning a smaller, efficient model (e.g., <code>gpt-4o-mini</code>), organizations can reduce dependency on larger, costlier models like <code>gpt-4o</code>, while still achieving high accuracy for specialized use cases.</p>
</section>
<section id="workflow-alignment" class="level4">
<h4 class="anchored" data-anchor-id="workflow-alignment"><strong>Workflow Alignment</strong></h4>
<p>Fine-tuning aligns the model’s behavior with domain-specific workflows, data structures, and terminology, ensuring seamless integration into business processes.</p>
<hr>
</section>
</section>
<section id="token-savings-through-shorter-prompts" class="level3">
<h3 class="anchored" data-anchor-id="token-savings-through-shorter-prompts">Token Savings Through Shorter Prompts</h3>
<p>Fine-tuned models inherently understand the domain context, reducing the need for lengthy and detailed instructions.</p>
<section id="reduced-prompt-length" class="level4">
<h4 class="anchored" data-anchor-id="reduced-prompt-length"><strong>Reduced Prompt Length</strong></h4>
<p>Pre-trained models often require extended prompts or few-shot examples to steer them toward accurate responses.<br>
Fine-tuned models need only minimal context, significantly reducing token usage.</p>
</section>
<section id="lower-api-costs" class="level4">
<h4 class="anchored" data-anchor-id="lower-api-costs"><strong>Lower API Costs</strong></h4>
<p>Since most LLM APIs are priced based on token usage, shorter prompts lead to cost savings and increased throughput.</p>
</section>
<section id="faster-performance" class="level4">
<h4 class="anchored" data-anchor-id="faster-performance"><strong>Faster Performance</strong></h4>
<p>Shorter prompts and smaller model sizes translate to lower latency, enabling near real-time responses—crucial for user-facing applications and high-frequency workloads.</p>
<hr>
</section>
</section>
<section id="improved-latency-and-user-experience" class="level3">
<h3 class="anchored" data-anchor-id="improved-latency-and-user-experience">Improved Latency and User Experience</h3>
<p>Fine-tuning not only enhances model accuracy but also contributes to faster response times:</p>
<ul>
<li><strong>Efficient Token Processing:</strong> With fewer tokens per request, models can process inputs more quickly.<br>
</li>
<li><strong>Improved Responsiveness:</strong> Applications backed by fine-tuned models provide a smoother, more interactive user experience.<br>
</li>
<li><strong>Scalability:</strong> Lower latency and reduced compute requirements make fine-tuned models more scalable for production use. ## Full Fine-Tuning Process: Step-by-Step Breakdown</li>
</ul>
<hr>
</section>
<section id="loading-pre-trained-model-weights" class="level3">
<h3 class="anchored" data-anchor-id="loading-pre-trained-model-weights">1. Loading Pre-Trained Model Weights</h3>
<section id="what-happens-behind-the-scenes" class="level4">
<h4 class="anchored" data-anchor-id="what-happens-behind-the-scenes">What Happens Behind the Scenes:</h4>
<ul>
<li>The model’s architecture (layers, dimensions, and connections) is initialized first.<br>
</li>
<li>Pre-trained weights are fetched from a model hub like Hugging Face.<br>
</li>
<li>These weights are loaded into corresponding layers of the initialized model.<br>
</li>
<li>Each weight and bias parameter holds knowledge learned from pre-training on massive corpora (e.g., Wikipedia, BookCorpus).</li>
</ul>
</section>
<section id="technical-details" class="level4">
<h4 class="anchored" data-anchor-id="technical-details">Technical Details:</h4>
<p>For a model like <strong>DistilBERT</strong>, approximately <strong>66 million parameters</strong> are loaded. These include:</p>
<ul>
<li>Token embeddings<br>
</li>
<li>Positional embeddings<br>
</li>
<li>Multi-head self-attention weights<br>
</li>
<li>Feed-forward network parameters<br>
</li>
<li>Layer normalization parameters</li>
</ul>
<p>These weights encode the model’s understanding of language syntax, semantics, and relationships.</p>
</section>
<section id="tensorflow-example" class="level4">
<h4 class="anchored" data-anchor-id="tensorflow-example">TensorFlow Example:</h4>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> TFAutoModelForSequenceClassification</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> TFAutoModelForSequenceClassification.from_pretrained(<span class="st">"distilbert-base-uncased"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="adding-a-task-specific-head-layer" class="level3">
<h3 class="anchored" data-anchor-id="adding-a-task-specific-head-layer">2. Adding a Task-Specific Head Layer</h3>
<section id="what-happens-behind-the-scenes-1" class="level4">
<h4 class="anchored" data-anchor-id="what-happens-behind-the-scenes-1">What Happens Behind the Scenes:</h4>
<ul>
<li>The original model typically ends with a general-purpose output layer<br>
</li>
<li>This gets replaced with a new layer designed specifically for your task<br>
</li>
<li>For classification, this is usually a linear layer with <strong>N outputs</strong> (where N = number of classes)<br>
</li>
<li>The new layer is initialized with random weights since it hasn’t been trained yet</li>
</ul>
</section>
<section id="technical-details-1" class="level4">
<h4 class="anchored" data-anchor-id="technical-details-1">Technical Details:</h4>
<ul>
<li>For sentiment classification with 2 classes, the head would be a <strong>Dense layer with 2 output neurons</strong><br>
</li>
<li>The head receives the <strong>pooled output from the final transformer layer</strong> (usually the <code>[CLS]</code> token representation)<br>
</li>
<li>In Hugging Face, this happens automatically when you specify <code>num_labels</code><br>
</li>
<li>The classification head is much smaller than the base model (typically <strong>&lt;1% of total parameters</strong>)</li>
</ul>
</section>
<section id="code-example" class="level4">
<h4 class="anchored" data-anchor-id="code-example">Code Example:</h4>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> TFAutoModelForSequenceClassification.from_pretrained(</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"distilbert-base-uncased"</span>,</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    num_labels<span class="op">=</span><span class="dv">2</span>  <span class="co"># Creates a classification head with 2 outputs</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
</section>
<section id="whats-happening-in-the-model-during-fine-tuning" class="level2">
<h2 class="anchored" data-anchor-id="whats-happening-in-the-model-during-fine-tuning">What’s Happening in the Model During Fine-tuning</h2>
<section id="layer-by-layer-adaptation" class="level3">
<h3 class="anchored" data-anchor-id="layer-by-layer-adaptation">Layer-by-Layer Adaptation:</h3>
<ol type="1">
<li><strong>Embedding Layer</strong>: Adjusts to prioritize domain-specific vocabulary and usage patterns<br>
</li>
<li><strong>Lower Transformer Layers</strong>: Refine basic pattern recognition for your domain’s language<br>
</li>
<li><strong>Middle Transformer Layers</strong>: Adapt semantic understanding to your specific context<br>
</li>
<li><strong>Upper Transformer Layers</strong>: Become more task-focused, identifying features relevant to classification<br>
</li>
<li><strong>Classification Head</strong>: Learns to map from contextual representations to your specific label space</li>
</ol>
<hr>
</section>
<section id="the-gradient-flow" class="level3">
<h3 class="anchored" data-anchor-id="the-gradient-flow">The Gradient Flow:</h3>
<ul>
<li>Different magnitudes of updates typically occur at different layers<br>
</li>
<li>Later layers usually change more dramatically than earlier layers<br>
</li>
<li>This creates a natural progression from general language understanding (lower layers) to task-specific processing (upper layers)</li>
</ul>
<hr>
</section>
<section id="concrete-example" class="level3">
<h3 class="anchored" data-anchor-id="concrete-example">Concrete Example</h3>
<p>Let’s see what’s happening in a sentiment classification task:</p>
<ol type="1">
<li><strong>Initial State</strong>
<ul>
<li>The model’s parameters can recognize general sentiment words (“good”, “bad”, “excellent”)<br>
</li>
<li>But it might not understand domain-specific indicators</li>
</ul></li>
<li><strong>During Fine-tuning</strong>
<ul>
<li>The embeddings gradually shift to better represent domain vocabulary<br>
(e.g., in movie reviews, <em>“gripping”</em> becomes more associated with positive sentiment)<br>
</li>
<li>Attention mechanisms learn to focus on the parts of sentences that carry sentiment in your domain<br>
</li>
<li>The classification head learns the boundary between your specific positive and negative examples</li>
</ul></li>
<li><strong>Final State</strong>
<ul>
<li>The model has:
<ul>
<li>Retained its general language understanding<br>
</li>
<li>Adapted to domain-specific vocabulary and patterns<br>
</li>
<li>Developed specialized attention patterns for your task<br>
</li>
<li>Learned classification boundaries specific to your labeled examples</li>
</ul></li>
</ul></li>
</ol>
<hr>
<p>This entire process enables the model to <strong>leverage its pre-trained knowledge</strong> while <strong>adapting specifically to your task and domain</strong>, ultimately resulting in better performance than either training from scratch or using the pre-trained model without fine-tuning.</p>
</section>
</section>
<section id="retrieval-augmented-generation-rag" class="level2">
<h2 class="anchored" data-anchor-id="retrieval-augmented-generation-rag">Retrieval Augmented Generation (RAG)</h2>
<section id="limitations-of-llms" class="level3">
<h3 class="anchored" data-anchor-id="limitations-of-llms">Limitations of LLMs</h3>
<ul>
<li>LLMs can’t answer correctly to private data<br>
</li>
<li>LLMs can’t provide the most current information</li>
</ul>
<p><strong>Example:</strong><br>
LLMs can’t provide information about a company’s private organizational data or a website we created and deployed.</p>
<hr>
<p>When we want to develop an application which can answer private data and current data, <strong>RAG is a solution</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/11_RAG_1.png" class="img-fluid figure-img"></p>
<figcaption>RAG Exmaple</figcaption>
</figure>
</div>
<ul>
<li><strong>RAG allows LLMs to use external sources for better performance.</strong><br>
</li>
<li>Whenever a user gives any kind of question related to private data, it will go to the <strong>Knowledge Base (KB)</strong>, and the KB will return a response based on the question.<br>
</li>
<li>The <strong>LLM will process</strong> that response along with the query and generate the correct answer.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/12_RAG_2.png" class="img-fluid figure-img"></p>
<figcaption>RAG Architecture</figcaption>
</figure>
</div>
</section>
</section>
<section id="retrieval-augmented-generation-rag-1" class="level2">
<h2 class="anchored" data-anchor-id="retrieval-augmented-generation-rag-1">Retrieval-Augmented Generation (RAG)</h2>
<p>RAG is a system designed to enhance LLMs by integrating them with a retrieval mechanism.</p>
<hr>
<section id="it-has-2-components" class="level3">
<h3 class="anchored" data-anchor-id="it-has-2-components">It Has 2 Components:</h3>
<section id="retriever-component" class="level4">
<h4 class="anchored" data-anchor-id="retriever-component">1. Retriever Component</h4>
<ul>
<li>Responsible for finding and accessing relevant information from external knowledge sources<br>
</li>
<li>When a question is submitted, the retriever analyzes the query to understand what information is needed and searches through the knowledge base (KB)<br>
</li>
<li>Identifies and pulls the most relevant documents, facts, and data points<br>
</li>
<li>Then prepares the retrieved information to be passed to the generator</li>
</ul>
</section>
<section id="generator-component" class="level4">
<h4 class="anchored" data-anchor-id="generator-component">2. Generator Component</h4>
<ul>
<li>The LLM receives both the original question and the context provided by the retriever<br>
</li>
<li>The LLM processes this combined input through its neural network<br>
</li>
<li>Synthesizes the information to formulate a coherent, relevant response<br>
</li>
<li>Finally, generates text that incorporates both its pre-trained knowledge and the retrieved context</li>
</ul>
<hr>
</section>
</section>
</section>
<section id="simple-rag-implementation" class="level2">
<h2 class="anchored" data-anchor-id="simple-rag-implementation">Simple RAG Implementation</h2>
<p>RAG typically works in <strong>2 Phases</strong>:</p>
<section id="indexing-phase" class="level3">
<h3 class="anchored" data-anchor-id="indexing-phase">1) Indexing Phase</h3>
<ul>
<li>Break documents into smaller chunks (e.g., paragraphs, sentences)<br>
</li>
<li>Convert each chunk into an embedding vector using a model<br>
</li>
<li>Store these chunks and vectors in a vector database</li>
</ul>
</section>
<section id="retrieval-and-generation-phase" class="level3">
<h3 class="anchored" data-anchor-id="retrieval-and-generation-phase">2) Retrieval and Generation Phase</h3>
<ul>
<li>Convert a user query into a vector using the same embedding model<br>
</li>
<li>Find chunks with the most similar vectors using cosine similarity<br>
</li>
<li>Pass the most relevant chunks to the LLM to generate a response</li>
</ul>
<hr>
</section>
<section id="main-components" class="level3">
<h3 class="anchored" data-anchor-id="main-components">3 Main Components</h3>
<ol type="1">
<li><p><strong>Embedding Model</strong><br>
Converts text into vector representations that capture semantic meaning</p></li>
<li><p><strong>Vector Database</strong><br>
Stores knowledge chunks and their corresponding vector representations</p></li>
<li><p><strong>LLM</strong><br>
A language model that generates responses using retrieved information</p></li>
</ol>
</section>
</section>
<section id="vector-databases" class="level2">
<h2 class="anchored" data-anchor-id="vector-databases">Vector Databases</h2>
<ul>
<li>Vector databases store both <strong>knowledge chunks</strong> and their <strong>vector representations</strong>.</li>
</ul>
<section id="knowledge-chunks" class="level3">
<h3 class="anchored" data-anchor-id="knowledge-chunks">Knowledge Chunks:</h3>
<ul>
<li>Pieces of text from source documents<br>
</li>
<li>RAG systems split the document into smaller, manageable pieces called “chunks”<br>
</li>
<li>These might be:
<ul>
<li>Individual paragraphs<br>
</li>
<li>Sentences<br>
</li>
<li>Fixed-length segments of text (e.g., 100 characters)</li>
</ul></li>
</ul>
</section>
<section id="vector-representations" class="level3">
<h3 class="anchored" data-anchor-id="vector-representations">Vector Representations:</h3>
<ul>
<li>Also called <strong>embeddings</strong>: numerical translations of text into a series of numbers<br>
</li>
<li>These numbers capture the <strong>semantic meaning</strong> of the text in a way that computers can process<br>
</li>
<li>Vectors typically have <strong>hundreds or thousands of dimensions</strong>, with each dimension representing some aspect of meaning learned by the embedding model</li>
</ul>
<hr>
</section>
<section id="vector-database-stores-pairs-of-items" class="level3">
<h3 class="anchored" data-anchor-id="vector-database-stores-pairs-of-items">Vector Database Stores Pairs of Items:</h3>
<ol type="1">
<li>The original text chunk<br>
</li>
<li>The vector embedding of that chunk</li>
</ol>
</section>
<section id="why-store-both" class="level3">
<h3 class="anchored" data-anchor-id="why-store-both">Why Store Both?</h3>
<ol type="1">
<li><strong>Efficiency</strong>
<ul>
<li>Storing embeddings allows for fast semantic searches</li>
</ul></li>
<li><strong>Search vs Retrieval</strong>
<ul>
<li>Vectors are used for <strong>searching</strong> (finding which chunks are semantically relevant to a query)<br>
</li>
<li>Original chunks are used for <strong>retrieving</strong> the actual content — shown to users or passed to the LLM</li>
</ul></li>
<li><strong>Context Preservation</strong>
<ul>
<li>Original text preserves <strong>important context and formatting</strong> that might be lost in the vector representation</li>
</ul></li>
</ol>
<hr>
</section>
</section>
<section id="cosine-similarity" class="level2">
<h2 class="anchored" data-anchor-id="cosine-similarity">Cosine Similarity</h2>
<ul>
<li>A mathematical function that measures how similar two vectors are<br>
</li>
<li><strong>Value range:</strong> 0 to 1 (for embeddings in positive space)<br>
</li>
<li><strong>Closer to 1 → higher similarity</strong></li>
</ul>
<section id="geometric-meaning" class="level3">
<h3 class="anchored" data-anchor-id="geometric-meaning">Geometric Meaning:</h3>
<ul>
<li>Cosine similarity measures the cosine of the angle between two vectors in multi-dimensional space</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 33%">
<col style="width: 46%">
</colgroup>
<thead>
<tr class="header">
<th>Cosine Value</th>
<th>Angle Between Vectors</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>0°</td>
<td>Vectors are identical</td>
</tr>
<tr class="even">
<td>0</td>
<td>90°</td>
<td>Vectors are perpendicular</td>
</tr>
<tr class="odd">
<td>-1</td>
<td>180°</td>
<td>Vectors point in opposite directions</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p><strong>Note</strong>: For most text embeddings (which are non-negative), cosine similarity values typically range from <strong>0 to 1</strong></p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/13_Cosine.png" class="img-fluid figure-img"></p>
<figcaption>Cosine Similarity</figcaption>
</figure>
</div>
</section>
<section id="cosine-similarity-formula" class="level3">
<h3 class="anchored" data-anchor-id="cosine-similarity-formula">Cosine Similarity Formula</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/15_Cosine_2.png" class="img-fluid figure-img"></p>
<figcaption>Cosine Similarity</figcaption>
</figure>
</div>
</section>
<section id="why-cosine-similarity-in-rag-systems" class="level3">
<h3 class="anchored" data-anchor-id="why-cosine-similarity-in-rag-systems">Why Cosine Similarity in RAG Systems?</h3>
<ol type="1">
<li><strong>Direction matters more than magnitude</strong>
<ul>
<li>It focuses on semantic meaning (vector direction), not scale<br>
</li>
</ul></li>
<li><strong>Normalization</strong>
<ul>
<li>Handles differences in document length since it is based on <strong>angle</strong>, not <strong>distance</strong><br>
</li>
</ul></li>
<li><strong>Efficiency</strong>
<ul>
<li>Fast to compute, especially with vector databases<br>
</li>
</ul></li>
<li><strong>Interpretability</strong>
<ul>
<li>Output range (0 to 1) is easy to interpret as a similarity percentage</li>
</ul></li>
</ol>
<hr>
</section>
<section id="example" class="level3">
<h3 class="anchored" data-anchor-id="example">Example</h3>
<p><strong>Query:</strong><br>
<code>"How fast do cats run?"</code> → <code>[0.8, 0.1, 0.5]</code></p>
<p><strong>Chunk 1:</strong><br>
<code>"Cats can travel at 31 mph"</code> → <code>[0.7, 0.2, 0.6]</code></p>
<p><strong>Chunk 2:</strong> …</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Images/14_Cosine_1.png" class="img-fluid figure-img"></p>
<figcaption>Cosine Similarity</figcaption>
</figure>
</div>
</section>
</section>
</section>

</main>
<!-- /main column -->
<div class="custom-footer">

  © 2025 Arundev Vamadevan — All rights reserved.

</div>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>